\section{Experimental validation}
Tests are executed on the Intel Xeon Phi KNC, with 64 physical cores with a four-way multithread support each, for a total of 256 threads.

For the text we have prepared an \textit{auto} mode that automatically runs the benchmarks for the provided input values. It runs the sequential, parallel and fastflow version, and for the latter two runs test with an increasing number of workers. At the end of the execution, the automatic mode provides a summary of the timestamps, in addition to those printed during the execution.

The time of each test is expressed in millisecond. Each timestamp provided in this document represent the averages of the timestamps of 5 tests performed with the same parameters.



\subsection{Sequential algorithm}
The sequential version with a window size $w = 100$, a tuple size $t = 10$, a sliding factor $k = 1$ and a stream length $l = 100000$ takes $\sim 24000 \ msec$ on the Intel Xeon Phi.

\bigskip\noindent
We also computed the partial times, measuring the time spent in the single operations with this parameter. We implemented the print as a delete of the skyline, since the actual print on standard out or file takes a very long time and is not scalable, hence we found it less interesting to be considered.

\medskip\noindent
Pick the windows from the stream: $\sim 88 \ msec$, i.e. $\sim 0.88 \ \mu sec$ each, i.e. $\sim 0,36\%$.\\
Process the windows: $\sim 24000 \ msec$, i.e. $\sim 240 \ \mu sec$ each, i.e. $\sim 99,39\%$.\\
Delete the skylines: $\sim 60 \ msec$, i.e. $\sim 0.6 \ \mu sec$ each, i.e. $\sim 0,25\%$.\\

\bigskip\noindent
Process a window takes $\sim 272$ times the time to pick it and $\sim 400$ times the time needed to delete the corresponding skyline. Since the Intel Xeon Phi supports up to 256 cores, we should be able to sped up the application by scaling on the second step, i.e. the computation of the skyline.



\subsection{Completion times}
The completion times are collected with a constant seed equal to $42$, so that at each run we obtain the same input stream. The tuple size $t$ is fixed at $10$ and the sliding factor $k$ at $1$. The variable parameters are the $nw$ (from $2^0$ to $2^8$) and the window size $w$ (from $2^7$ to $2^{11}$). Since we want always the same number of window $w\_no = 12800$ (that is $50$ window per worker with $256$ workers), the stream length changes with the window size $w$. Hence, because $w\_no = \lfloor\frac{l-w}{k}\rfloor+1$, with a fixed $w\_no$ the stream size will be $l = (w\_no - 1) \cdot k + w$. For example, with $w = 128$ we have $l = 12927$.

\begin{table}[H]
    \centering
    \input{assets/time_table}
    \caption{Completion times}
    \label{tab:time_table}
\end{table}

\noindent
We can observe that with a bigger window size $w$ our parallel implementations scale better, since the workload of each task is bigger and the overhead is spread over longer times and better mitigated.

\begin{figure}[H]
    \centering
    \includegraphics[width=.8\linewidth]{assets/completion_time}
    \caption{Completion time chart in a logarithmic scale on the y axis, $nw$ on the x axis}
    \label{fig:completion_time}
\end{figure}

\noindent
In the Figure \ref{fig:completion_time} we see clearly how the lines tagged as $w = 128$ and $w = 256$ descend with a low $nw$ but than ascend when the $nw$ increases.

This behaviour affects the speedup and the scalability, as we can notice in the following sections.



\subsection{Speedup}
The \textit{speedup} is computed as $s(p) = \frac{T_{seq}}{T_{par}(p)}$.

\begin{figure}[H]
    \centering
    \includegraphics[width=.8\linewidth]{assets/speedup}
    \bigskip
    \caption{Speedup chart}
    \label{fig:speedup}
\end{figure}

\noindent
In the Figure \ref{fig:speedup} we can see how the C++ threads version is better than the \textit{FastFlow} one. Also, with the C++ threads only execution with $w = 128$ stop gain (and start lose) before reaching the $256$ workers, that is the machine limit. The \textit{FastFlow} version instead stop gain also with $w = 256$. Moreover, the \textit{FastFlow} implementation need more thread to reach the same speedup of the C++ threads one, and hence we can notice that with $w = 128$ the \textit{FastFlow} speedup start decreasing between $128$ and $256$ workers, C++ Threads instead between $64$ and $128$ but after reaching a higher speedup value.



\subsection{Scalability}
The scalability is computed as $scalab(p) = \frac{T_{par}(1)}{T_{par}(p)}$.

\begin{figure}[H]
    \centering
    \includegraphics[width=.8\linewidth]{assets/scalability}
    \caption{Scalability chart}
    \label{fig:scalability}
\end{figure}

\noindent
The scalability is pretty similar to the speedup of our application. Again, the C++ Threads version is better than the \textbf{FastFlow} one. Also, there is a strange behavior of \textbf{FastFlow}: while the C++ Threads graph has an "S"-shaped curve, in this one we can notice worsening between $32$ and $64$ threads, but then there is a new improvement and the function continues almost linearly. This is not as good as the ideal one, that is exponential, but may mean that at some point, with a higher number of cores, using \textit{FastFlow} can be more convinient than our C++ Thread implementation.



\subsection{Efficiency}
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\linewidth]{assets/efficiency}
    \caption{Efficiency chart}
    \label{fig:efficiency}
\end{figure}