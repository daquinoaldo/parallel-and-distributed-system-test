\section{Experimental validation}
Tests are executed on the Intel Xeon Phi KNC, with 64 physical cores with a four-way multi-thread support each, for a total of 256 threads.

For the text we have prepared an \textit{auto} mode that automatically runs the benchmarks for the provided input values. It runs the sequential, parallel and FastFlow version, and for the latter two runs test with an increasing number of workers. At the end of the execution, the automatic mode provides a summary of the timestamps, in addition to those printed during the execution.

The time of each test is expressed in millisecond. Each timestamp provided in this document represent the averages of the timestamps of 5 tests performed with the same parameters.



\subsection{Sequential algorithm}
The sequential version with a window size $w = 100$, a tuple size $t = 10$, a sliding factor $k = 1$ and a stream length $l = 100000$ takes $\sim 24000 \ msec$ on the Intel Xeon Phi.

\bigskip\noindent
We also computed the partial times, measuring the time spent in the single operations with this parameter. We implemented the print as a delete of the skyline, since the actual print on standard out or file takes a very long time and is not scalable, hence we found it less interesting to be considered.

\medskip\noindent
Pick the windows from the stream: $\sim 88 \ msec$, i.e. $\sim 0.88 \ \mu sec$ each, i.e. $\sim 0,36\%$.\\
Process the windows: $\sim 24000 \ msec$, i.e. $\sim 240 \ \mu sec$ each, i.e. $\sim 99,39\%$.\\
Delete the skylines: $\sim 60 \ msec$, i.e. $\sim 0.6 \ \mu sec$ each, i.e. $\sim 0,25\%$.\\

\bigskip\noindent
Process a window takes $\sim 272$ times the time to pick it and $\sim 400$ times the time needed to delete the corresponding skyline. Since the Intel Xeon Phi supports up to 256 cores, we should be able to sped up the application by scaling on the second step, i.e. the computation of the skyline.



\subsection{Completion times}
The completion times are collected with a constant seed equal to $42$, so that at each run we obtain the same input stream. The tuple size $t$ is fixed at $10$ and the sliding factor $k$ at $1$. The variable parameters are the $nw$ (from $2^0$ to $2^8$) and the window size $w$ (from $2^7$ to $2^{11}$). Since we want always the same number of window $w\_no = 12800$ (that is $50$ window per worker with $256$ workers), the stream length changes with the window size $w$. Hence, because $w\_no = \lfloor\frac{l-w}{k}\rfloor+1$, with a fixed $w\_no$ the stream size will be $l = (w\_no - 1) \cdot k + w$. For example, with $w = 128$ we have $l = 12927$.

\begin{table}[H]
    \centering
    \input{assets/time_table}
    \caption{Completion times of the sequential version to vary the window size $w$ and of the parallel C++ threads and \textit{FastFlow} versions to vary the parallel degree $nw$ and the window size $w$.}
    \label{tab:time_table}
\end{table}

\noindent
We can observe that with a bigger window size $w$ our parallel implementations scale better, since the workload of each task is bigger and the overhead is spread over longer times and better mitigated.

\begin{figure}[H]
    \centering
    \includegraphics[width=.75\linewidth]{assets/completion_time}
    \caption{Completion time chart in a logarithmic scale on the y axis, $nw$ on the x axis.}
    \label{fig:completion_time}
\end{figure}

\noindent
In the Figure \ref{fig:completion_time} we see clearly how the lines tagged as $w = 128$ and $w = 256$ descend with a low $nw$ but than ascend when the $nw$ increases.

This behaviour affects the speedup and the scalability, as we can notice in the following sections.



\subsection{Speedup}
The \textit{speedup} is computed as $s(p) = \frac{T_{seq}}{T_{par}(p)}$. Ideally, the \textit{speedup} should be the same of the parallel degree $p$.

\begin{figure}[H]
    \centering
    \begin{minipage}{.68\linewidth}
        \includegraphics[width=\linewidth]{assets/speedup}
    \end{minipage}
    \begin{minipage}{.3\linewidth}
        \includegraphics[width=\linewidth]{assets/speedup_ideal}
    \end{minipage}
    \caption{Speedup chart, on the right with the ideal speedup as reference.}
    \label{fig:speedup}
\end{figure}

\bigskip\noindent
In the figure \ref{fig:speedup} we can see how the C++ threads version is better than the \textit{FastFlow} one. Also, with the C++ threads only execution with $w = 128$ stop gain (and start lose) before reaching the $256$ workers, that is the machine limit. The \textit{FastFlow} version instead stop gain also with $w = 256$. Moreover, the \textit{FastFlow} implementation need more thread to reach the same speedup of the C++ threads one, and hence we can notice that with $w = 128$ the \textit{FastFlow} speedup start decreasing between $128$ and $256$ workers, C++ Threads instead between $64$ and $128$ but after reaching a higher speedup value.



\subsection{Scalability}
The \textit{scalability} is computed as $scalab(p) = \frac{T_{par}(1)}{T_{par}(p)}$. Ideally the \textit{scalability} should be the same of the parallel degree $p$.

\begin{figure}[H]
    \centering
    \begin{minipage}{.68\linewidth}
        \includegraphics[width=\linewidth]{assets/scalability}
    \end{minipage}
    \begin{minipage}{.3\linewidth}
        \includegraphics[width=\linewidth]{assets/scalability_ideal}
    \end{minipage}
    \caption{Scalability chart, on the right with the ideal scalability as reference.}
    \label{fig:scalability}
\end{figure}

\noindent
The scalability is pretty similar to the speedup of our application. Again, the C++ Threads version is better than the \textbf{FastFlow} one. Also, there is a strange behavior of \textbf{FastFlow}: while the C++ Threads graph has an "S"-shaped curve, in this one we can notice worsening between $32$ and $64$ threads, but then there is a new improvement and the function continues almost linearly. This is not as good as the ideal one, that is exponential, but may mean that at some point, with a higher number of cores, using \textit{FastFlow} can be more convenient than our C++ Thread implementation.



\subsection{Efficiency}
The \textit{efficiency} is computed as $\epsilon(p) = \frac{T_{seq}}{p\ \cdot\ T_{par}(p)}$. In efficient implementation we have $\epsilon = 1$ regardless of the parallel degree $p$.

\begin{figure}[H]
    \centering
    \includegraphics[width=.75\linewidth]{assets/efficiency_ideal}
    \caption{Efficiency chart.}
    \label{fig:efficiency}
\end{figure}

\noindent
We can clearly see in the figure \ref{fig:efficiency} that we have a high efficiency up to $16$ - $32$ threads, and then starts decreasing drastically. Increasing the window size $w$, the efficiency start decreasing at a higher parallel degree $nw$.